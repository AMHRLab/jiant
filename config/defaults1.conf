// Default config file.
// Set values here, then override them in custom configs by including this at
// the top, e.g.
// my_expt.conf:
include "defaults.conf"

// Paths and logging
// For local run, override this with ${JIANT_PROJECT_PREFIX}
//project_dir = ${NFS_PROJECT_PREFIX}  // for export to NFS
project_dir = ${JIANT_PROJECT_PREFIX}  // uncomment for local/testing
exp_name = "common-indexed-tasks"  // experiment name
run_name = "defaults1"  // run name
exp_dir = ${project_dir}"/"${exp_name}"/"  // required
run_dir = ${project_dir}"/"${exp_name}"/"${run_name}  // required
log_file = "log.log"  // log file, goes in run directory
// log name for remote logging; make as unique as possible
remote_log_name = ${exp_name}"__"${run_name}
// Data paths
data_dir = ${JIANT_DATA_DIR}  // required
global_ro_exp_dir = "/nfs/jsalt/share/exp/default"

// Execution control
do_train = 1    // run training steps
do_eval = 0     // run eval steps
load_model = 0  // if true, restore main training from checkpoint if available
load_eval_checkpoint = "none"  // path to eval checkpoint, or "none"
skip_task_models = 0 // if true, only load task-independent parameters when using load_eval_checkpoint
reload_tasks = 0     // if true, force reloading tasks
reload_indexing = 0  // if true, force reindexing tasks
reload_vocab = 0     // if true, force vocabulary rebuild

// Tasks and task-specific modules
train_tasks = glue
//"sst,mrpc,rte"
// required, comma-separated list of training tasks,
                      // or 'all' or 'none'
eval_tasks = glue   // required, additional eval tasks
train_for_eval = 0  // if true and if needed, train new classifiers for eval tasks


// Training options
no_tqdm = 1  // if true, disable tqdm progress bar
trainer_type = "sampling"  // type of trainer: 'sampling'
shared_optimizer = 1  // if true, use same optimizer for all tasks
lr = 0.0001          // initial learning rate
min_lr = 0.000001  // (1e-6) minimum learning rate
max_grad_norm = 5.0  // maximum gradient norm
weight_decay = 0.0000001   // weight decay value
task_patience = 1    // patience in decaying per-task learning rate
scheduler_threshold = 0.0001 // threshold used in deciding when to lower learning rate
lr_decay_factor = 0.5  // learning rate decay factor, when validation score
                       // doesn't improve
warmup = 4000  // number of warmup steps for transformer LR schedule
bpp_base = 1       // number of batches to train per sampled task
patience = 5  // patience in early stopping
keep_all_checkpoints = 0 // If set, keep checkpoints from every validation.
                         // Otherwise, keep only best and (if different) most recent.

batch_size = 8   // training batch size
val_interval = 100  // number of passes between validation checks
max_vals = 5     // maximum number of validation checks

// Multi-task training options
weighting_method = "uniform"  // weighting method for sampling:
                                   // 'uniform' or 'proportional'
scaling_method = "none"   // method for scaling loss:
                         // 'min', 'max', 'unit', or 'none'
