06/29 09:58:24 PM: fastText library not found!
06/29 09:58:24 PM: Parsed args: 
{
  "batch_size": 1024,
  "bidirectional": 1,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 512,
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 512,
  "cola_lr": 0.0001,
  "cola_max_vals": 25,
  "cola_val_interval": 100,
  "cove": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 512,
  "d_hid_attn": 512,
  "d_hid_dec": 300,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 300,
  "data_dir": "/usr/share/jsalt/glue_data",
  "do_eval": 1,
  "do_train": 1,
  "dropout": 0.2,
  "dropout_embs": 0.2,
  "elmo": 1,
  "elmo_chars_only": 1,
  "eval_max_vals": 50,
  "eval_tasks": "sst",
  "eval_val_interval": 500,
  "exp_dir": "/home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/",
  "exp_name": "jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200",
  "fastText": 0,
  "fastText_model_file": ".",
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 1,
  "log_file": "log.log",
  "lr": 0.0001,
  "lr_decay_factor": 0.5,
  "max_char_v_size": 250,
  "max_grad_norm": 5.0,
  "max_seq_len": 40,
  "max_vals": 25,
  "max_word_v_size": 30000,
  "min_lr": 1e-06,
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0001,
  "mnli_max_vals": 15,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 10000,
  "model_size": 200,
  "mrpc_classifier_dropout": 0.4,
  "mrpc_classifier_hid_dim": 128,
  "mrpc_lr": 0.0001,
  "mrpc_max_vals": 50,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_dec": 1,
  "n_layers_enc": 2,
  "n_layers_highway": 0,
  "no_tqdm": 1,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "preproc_file": "preproc.pkl",
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0001,
  "qnli_max_vals": 25,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 10000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0001,
  "qqp_max_vals": 15,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 10000,
  "random_seed": 19,
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200__sst",
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_lr": 0.0001,
  "rte_max_vals": 50,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "/home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/sst",
  "run_name": "sst",
  "scaling_method": "max",
  "scheduler_threshold": 0.0001,
  "sent_combine_method": "max",
  "sent_enc": "transformer",
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 1,
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 512,
  "sst_lr": 0.0001,
  "sst_max_vals": 25,
  "sst_val_interval": 500,
  "sts-b_classifier_dropout": 0.4,
  "sts-b_classifier_hid_dim": 128,
  "sts-b_lr": 0.0001,
  "sts-b_max_vals": 50,
  "sts-b_pair_attn": 0,
  "sts-b_val_interval": 100,
  "task_patience": 2,
  "train_for_eval": 1,
  "train_tasks": "sst",
  "trainer_type": "sampling",
  "val_interval": 5000,
  "warmup": 4000,
  "weight_decay": 1e-07,
  "weighting_method": "proportional",
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_lr": 0.0001,
  "wnli_max_vals": 50,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "none",
  "word_embs_file": "/usr/share/jsalt/glove/glove.840B.300d.txt",
  "write_preds": 0
}
06/29 09:58:24 PM: Saved config to /home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/sst/params.conf
06/29 09:58:24 PM: Using random seed 19
06/29 09:58:24 PM: Using GPU 0
06/29 09:58:24 PM: Loading tasks...
06/29 09:58:24 PM: Writing pre-preprocessed tasks to /home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/
06/29 09:58:24 PM: 	Loaded existing task sst
06/29 09:58:24 PM: 	Finished loading tasks: sst.
06/29 09:58:24 PM: Loading token dictionary from /home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/vocab.
06/29 09:58:24 PM: 	Loaded vocab from /home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/vocab
06/29 09:58:24 PM: 	Finished building vocab. Using 15707 words, 74 chars.
06/29 09:58:24 PM: 	Finished indexing tasks
06/29 09:58:24 PM: 	Lazy-loading indexed data for task='sst' from /home/raghu1991_p_gmail_com/exp/jsalt_demo_sst_Transformer_default_conf_SingleTask_2EncLayers_v2_Batch1024_Modelsize200/preproc
06/29 09:58:24 PM: All tasks initialized with data iterators.
06/29 09:58:24 PM: 	  Training on sst
06/29 09:58:24 PM: 	  Evaluating on sst
06/29 09:58:24 PM: 	Finished loading tasks in 0.142s
06/29 09:58:24 PM: Building model...
06/29 09:58:24 PM: 	Not using word embeddings!
06/29 09:58:24 PM: 	Not using character embeddings!
06/29 09:58:24 PM: Loading ELMo from files:
06/29 09:58:24 PM: ELMO_OPT_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json
06/29 09:58:24 PM: ELMO_WEIGHTS_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5
06/29 09:58:24 PM: 	Using ELMo character CNN only!
06/29 09:58:29 PM: input_dim = 512
06/29 09:58:29 PM: hidden_dim = 512
06/29 09:58:29 PM: projection_dim = 64
06/29 09:58:29 PM: feedforward_hidden_dim = 2048
06/29 09:58:29 PM: num_layers = 2
06/29 09:58:29 PM: num_attention_heads = 8
06/29 09:58:29 PM: use_positional_encoding = True
06/29 09:58:29 PM: dropout_prob = 0.1
06/29 09:58:29 PM: residual_dropout_prob = 0.2
06/29 09:58:29 PM: attention_dropout_prob = 0.1
06/29 09:58:29 PM: Initializing parameters
06/29 09:58:29 PM: Done initializing parameters; the following parameters are using their default initialization from their code
06/29 09:58:29 PM:    _phrase_layer.feedforward_0._linear_layers.0.bias
06/29 09:58:29 PM:    _phrase_layer.feedforward_0._linear_layers.0.weight
06/29 09:58:29 PM:    _phrase_layer.feedforward_0._linear_layers.1.bias
06/29 09:58:29 PM:    _phrase_layer.feedforward_0._linear_layers.1.weight
06/29 09:58:29 PM:    _phrase_layer.feedforward_1._linear_layers.0.bias
06/29 09:58:29 PM:    _phrase_layer.feedforward_1._linear_layers.0.weight
06/29 09:58:29 PM:    _phrase_layer.feedforward_1._linear_layers.1.bias
06/29 09:58:29 PM:    _phrase_layer.feedforward_1._linear_layers.1.weight
06/29 09:58:29 PM:    _phrase_layer.feedforward_layer_norm_0.beta
06/29 09:58:29 PM:    _phrase_layer.feedforward_layer_norm_0.gamma
06/29 09:58:29 PM:    _phrase_layer.feedforward_layer_norm_1.beta
06/29 09:58:29 PM:    _phrase_layer.feedforward_layer_norm_1.gamma
06/29 09:58:29 PM:    _phrase_layer.layer_norm_0.beta
06/29 09:58:29 PM:    _phrase_layer.layer_norm_0.gamma
06/29 09:58:29 PM:    _phrase_layer.layer_norm_1.beta
06/29 09:58:29 PM:    _phrase_layer.layer_norm_1.gamma
06/29 09:58:29 PM:    _phrase_layer.self_attention_0._combined_projection.bias
06/29 09:58:29 PM:    _phrase_layer.self_attention_0._combined_projection.weight
06/29 09:58:29 PM:    _phrase_layer.self_attention_0._output_projection.bias
06/29 09:58:29 PM:    _phrase_layer.self_attention_0._output_projection.weight
06/29 09:58:29 PM:    _phrase_layer.self_attention_1._combined_projection.bias
06/29 09:58:29 PM:    _phrase_layer.self_attention_1._combined_projection.weight
06/29 09:58:29 PM:    _phrase_layer.self_attention_1._output_projection.bias
06/29 09:58:29 PM:    _phrase_layer.self_attention_1._output_projection.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._char_embedding_weights
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._projection.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo._projection.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_0.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_0.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_1.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_1.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_2.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_2.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_3.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_3.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_4.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_4.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_5.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_5.weight
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_6.bias
06/29 09:58:29 PM:    _text_field_embedder.token_embedder_elmo.char_conv_6.weight
> [0;32m/home/raghu1991_p_gmail_com/jiant_v2/src/models.py[0m(110)[0;36mbuild_model[0;34m()[0m
[0;32m    109 [0;31m    [0mpdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[0;32m--> 110 [0;31m    [0mbuild_modules[0m[0;34m([0m[0mtasks[0m[0;34m,[0m [0mmodel[0m[0;34m,[0m [0md_sent[0m[0;34m,[0m [0mvocab[0m[0;34m,[0m [0membedder[0m[0;34m,[0m [0margs[0m[0;34m)[0m[0;34m[0m[0m
[0m[0;32m    111 [0;31m    [0;32mif[0m [0margs[0m[0;34m.[0m[0mcuda[0m [0;34m>=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[0m
ipdb> MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BasicTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): StackedSelfAttentionEncoder(
      (feedforward_0): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_0): LayerNorm()
      (self_attention_0): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_0): LayerNorm()
      (feedforward_1): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_1): LayerNorm()
      (self_attention_1): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_1): LayerNorm()
      (dropout): Dropout(p=0.2)
    )
    (_dropout): Dropout(p=0.2)
  )
)
ipdb> ['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_all_buffers', '_apply', '_backend', '_backward_hooks', '_buffers', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_grounded_classification_forward', '_lm_forward', '_load_from_state_dict', '_modules', '_pair_sentence_forward', '_parameters', '_ranking_forward', '_seq_gen_forward', '_single_sentence_forward', '_slow_forward', '_tracing_name', '_version', 'add_module', 'apply', 'children', 'combine_method', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'load_state_dict', 'modules', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'sent_encoder', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'vocab', 'zero_grad']
ipdb> <bound method Module.named_modules of MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BasicTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): StackedSelfAttentionEncoder(
      (feedforward_0): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_0): LayerNorm()
      (self_attention_0): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_0): LayerNorm()
      (feedforward_1): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_1): LayerNorm()
      (self_attention_1): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_1): LayerNorm()
      (dropout): Dropout(p=0.2)
    )
    (_dropout): Dropout(p=0.2)
  )
)>
ipdb> <bound method Module.modules of MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BasicTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): StackedSelfAttentionEncoder(
      (feedforward_0): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_0): LayerNorm()
      (self_attention_0): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_0): LayerNorm()
      (feedforward_1): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_1): LayerNorm()
      (self_attention_1): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_1): LayerNorm()
      (dropout): Dropout(p=0.2)
    )
    (_dropout): Dropout(p=0.2)
  )
)>
ipdb> *** SyntaxError: unexpected EOF while parsing
ipdb> sent_encoder
ipdb> sent_encoder
ipdb> 1
2
3
ipdb> 0 MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BasicTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): StackedSelfAttentionEncoder(
      (feedforward_0): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_0): LayerNorm()
      (self_attention_0): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_0): LayerNorm()
      (feedforward_1): FeedForward(
        (_linear_layers): ModuleList(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): Linear(in_features=2048, out_features=512, bias=True)
        )
        (_dropout): ModuleList(
          (0): Dropout(p=0.1)
          (1): Dropout(p=0.1)
        )
      )
      (feedforward_layer_norm_1): LayerNorm()
      (self_attention_1): MultiHeadSelfAttention(
        (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
        (_output_projection): Linear(in_features=64, out_features=512, bias=True)
        (_attention_dropout): Dropout(p=0.1)
      )
      (layer_norm_1): LayerNorm()
      (dropout): Dropout(p=0.2)
    )
    (_dropout): Dropout(p=0.2)
  )
)
1 SentenceEncoder(
  (_text_field_embedder): BasicTextFieldEmbedder(
    (token_embedder_elmo): ElmoCharacterEncoder(
      (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
      (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
      (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
      (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
      (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
      (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
      (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
      (_highways): Highway(
        (_layers): ModuleList(
          (0): Linear(in_features=2048, out_features=4096, bias=True)
          (1): Linear(in_features=2048, out_features=4096, bias=True)
        )
      )
      (_projection): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (_highway_layer): TimeDistributed(
    (_module): Highway(
      (_layers): ModuleList()
    )
  )
  (_phrase_layer): StackedSelfAttentionEncoder(
    (feedforward_0): FeedForward(
      (_linear_layers): ModuleList(
        (0): Linear(in_features=512, out_features=2048, bias=True)
        (1): Linear(in_features=2048, out_features=512, bias=True)
      )
      (_dropout): ModuleList(
        (0): Dropout(p=0.1)
        (1): Dropout(p=0.1)
      )
    )
    (feedforward_layer_norm_0): LayerNorm()
    (self_attention_0): MultiHeadSelfAttention(
      (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
      (_output_projection): Linear(in_features=64, out_features=512, bias=True)
      (_attention_dropout): Dropout(p=0.1)
    )
    (layer_norm_0): LayerNorm()
    (feedforward_1): FeedForward(
      (_linear_layers): ModuleList(
        (0): Linear(in_features=512, out_features=2048, bias=True)
        (1): Linear(in_features=2048, out_features=512, bias=True)
      )
      (_dropout): ModuleList(
        (0): Dropout(p=0.1)
        (1): Dropout(p=0.1)
      )
    )
    (feedforward_layer_norm_1): LayerNorm()
    (self_attention_1): MultiHeadSelfAttention(
      (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
      (_output_projection): Linear(in_features=64, out_features=512, bias=True)
      (_attention_dropout): Dropout(p=0.1)
    )
    (layer_norm_1): LayerNorm()
    (dropout): Dropout(p=0.2)
  )
  (_dropout): Dropout(p=0.2)
)
2 BasicTextFieldEmbedder(
  (token_embedder_elmo): ElmoCharacterEncoder(
    (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
    (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
    (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
    (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
    (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
    (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
    (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
    (_highways): Highway(
      (_layers): ModuleList(
        (0): Linear(in_features=2048, out_features=4096, bias=True)
        (1): Linear(in_features=2048, out_features=4096, bias=True)
      )
    )
    (_projection): Linear(in_features=2048, out_features=512, bias=True)
  )
)
3 ElmoCharacterEncoder(
  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
  (_highways): Highway(
    (_layers): ModuleList(
      (0): Linear(in_features=2048, out_features=4096, bias=True)
      (1): Linear(in_features=2048, out_features=4096, bias=True)
    )
  )
  (_projection): Linear(in_features=2048, out_features=512, bias=True)
)
4 Conv1d(16, 32, kernel_size=(1,), stride=(1,))
5 Conv1d(16, 32, kernel_size=(2,), stride=(1,))
6 Conv1d(16, 64, kernel_size=(3,), stride=(1,))
7 Conv1d(16, 128, kernel_size=(4,), stride=(1,))
8 Conv1d(16, 256, kernel_size=(5,), stride=(1,))
9 Conv1d(16, 512, kernel_size=(6,), stride=(1,))
10 Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
11 Highway(
  (_layers): ModuleList(
    (0): Linear(in_features=2048, out_features=4096, bias=True)
    (1): Linear(in_features=2048, out_features=4096, bias=True)
  )
)
12 ModuleList(
  (0): Linear(in_features=2048, out_features=4096, bias=True)
  (1): Linear(in_features=2048, out_features=4096, bias=True)
)
13 Linear(in_features=2048, out_features=4096, bias=True)
14 Linear(in_features=2048, out_features=4096, bias=True)
15 Linear(in_features=2048, out_features=512, bias=True)
16 TimeDistributed(
  (_module): Highway(
    (_layers): ModuleList()
  )
)
17 Highway(
  (_layers): ModuleList()
)
18 ModuleList()
19 StackedSelfAttentionEncoder(
  (feedforward_0): FeedForward(
    (_linear_layers): ModuleList(
      (0): Linear(in_features=512, out_features=2048, bias=True)
      (1): Linear(in_features=2048, out_features=512, bias=True)
    )
    (_dropout): ModuleList(
      (0): Dropout(p=0.1)
      (1): Dropout(p=0.1)
    )
  )
  (feedforward_layer_norm_0): LayerNorm()
  (self_attention_0): MultiHeadSelfAttention(
    (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
    (_output_projection): Linear(in_features=64, out_features=512, bias=True)
    (_attention_dropout): Dropout(p=0.1)
  )
  (layer_norm_0): LayerNorm()
  (feedforward_1): FeedForward(
    (_linear_layers): ModuleList(
      (0): Linear(in_features=512, out_features=2048, bias=True)
      (1): Linear(in_features=2048, out_features=512, bias=True)
    )
    (_dropout): ModuleList(
      (0): Dropout(p=0.1)
      (1): Dropout(p=0.1)
    )
  )
  (feedforward_layer_norm_1): LayerNorm()
  (self_attention_1): MultiHeadSelfAttention(
    (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
    (_output_projection): Linear(in_features=64, out_features=512, bias=True)
    (_attention_dropout): Dropout(p=0.1)
  )
  (layer_norm_1): LayerNorm()
  (dropout): Dropout(p=0.2)
)
20 FeedForward(
  (_linear_layers): ModuleList(
    (0): Linear(in_features=512, out_features=2048, bias=True)
    (1): Linear(in_features=2048, out_features=512, bias=True)
  )
  (_dropout): ModuleList(
    (0): Dropout(p=0.1)
    (1): Dropout(p=0.1)
  )
)
21 ModuleList(
  (0): Linear(in_features=512, out_features=2048, bias=True)
  (1): Linear(in_features=2048, out_features=512, bias=True)
)
22 Linear(in_features=512, out_features=2048, bias=True)
23 Linear(in_features=2048, out_features=512, bias=True)
24 ModuleList(
  (0): Dropout(p=0.1)
  (1): Dropout(p=0.1)
)
25 Dropout(p=0.1)
26 Dropout(p=0.1)
27 LayerNorm()
28 MultiHeadSelfAttention(
  (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
  (_output_projection): Linear(in_features=64, out_features=512, bias=True)
  (_attention_dropout): Dropout(p=0.1)
)
29 Linear(in_features=512, out_features=192, bias=True)
30 Linear(in_features=64, out_features=512, bias=True)
31 Dropout(p=0.1)
32 LayerNorm()
33 FeedForward(
  (_linear_layers): ModuleList(
    (0): Linear(in_features=512, out_features=2048, bias=True)
    (1): Linear(in_features=2048, out_features=512, bias=True)
  )
  (_dropout): ModuleList(
    (0): Dropout(p=0.1)
    (1): Dropout(p=0.1)
  )
)
34 ModuleList(
  (0): Linear(in_features=512, out_features=2048, bias=True)
  (1): Linear(in_features=2048, out_features=512, bias=True)
)
35 Linear(in_features=512, out_features=2048, bias=True)
36 Linear(in_features=2048, out_features=512, bias=True)
37 ModuleList(
  (0): Dropout(p=0.1)
  (1): Dropout(p=0.1)
)
38 Dropout(p=0.1)
39 Dropout(p=0.1)
40 LayerNorm()
41 MultiHeadSelfAttention(
  (_combined_projection): Linear(in_features=512, out_features=192, bias=True)
  (_output_projection): Linear(in_features=64, out_features=512, bias=True)
  (_attention_dropout): Dropout(p=0.1)
)
42 Linear(in_features=512, out_features=192, bias=True)
43 Linear(in_features=64, out_features=512, bias=True)
44 Dropout(p=0.1)
45 LayerNorm()
46 Dropout(p=0.2)
47 Dropout(p=0.2)
ipdb> *** SyntaxError: invalid syntax
ipdb> 06/29 11:01:19 PM: Fatal error in main():
Traceback (most recent call last):
  File "src/main.py", line 184, in <module>
    main(sys.argv[1:])
  File "src/main.py", line 94, in main
    model = build_model(args, vocab, word_embs, tasks)
  File "/home/raghu1991_p_gmail_com/jiant_v2/src/models.py", line 110, in build_model
    build_modules(tasks, model, d_sent, vocab, embedder, args)
  File "/home/raghu1991_p_gmail_com/jiant_v2/src/models.py", line 110, in build_model
    build_modules(tasks, model, d_sent, vocab, embedder, args)
  File "/usr/share/anaconda3/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/share/anaconda3/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

